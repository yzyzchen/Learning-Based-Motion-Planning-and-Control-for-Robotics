{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# ROB 498: Robot Learning for Planning and Control\n", "# Assignment 3: Learning Dynamics Directly from State Measurements"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Colab Setup"], "outputs": []}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["# TODO: Fill in the Google Drive path where you uploaded the assignment\n", "# Example: If you create a ROB498 folder and put all the files under HW3 folder, then 'ROB498/HW3'\n", "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ROB498/HW3'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Setup Code "], "outputs": []}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from google.colab import drive\n", "\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["import os\n", "import sys\n", "\n", "# GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n", "GOOGLE_DRIVE_PATH = '/Users/mik/Work/ROB498/rob498_learning_for_planning_and_control/Assignments/HW3' # TODO: Remove this line and uncomment the one above\n", "# GOOGLE_DRIVE_PATH = '/home/tpower/dev/teaching/ROB498_learning_for_planning/rob498_learning_for_planning_and_control/Assignments/HW3'\n", "\n", "\n", "files = os.listdir(GOOGLE_DRIVE_PATH)\n", "expected_files = [ 'ROB498_hw3.ipynb', 'learning_state_dynamics.py', 'panda_pushing_env.py' 'visualizers.py',  'mppi.py', 'validation_data.py']\n", "\n", "sys.path.append(GOOGLE_DRIVE_PATH)\n", "\n", "# Verify that there are all the expected files in the directory\n", "all_found = True\n", "for expected_file in expected_files:\n", "  if expected_file not in files:\n", "    print(f'Required file {expected_file} not found!')\n", "    all_found = False\n", "if all_found:\n", "  print('All required files are found :)')"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["# Install missing required packages \n", "# Unfortunately Colab does not have pybullet package by default, so we will have to install it every time that the notebook kernel is restarted.\n", "# Install pybullet -- For simulation purposes\n", "!pip install pybullet\n", "# Install numpngw -- For visualization purposes\n", "!pip install numpngw"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["import torch\n", "import os\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "import numpy as np\n", "from torch.utils.data import Dataset, DataLoader\n", "import matplotlib.pyplot as plt\n", "from numpngw import write_apng\n", "from IPython.display import Image\n", "from tqdm.notebook import tqdm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Assignment Introduction"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Due 3/8 at 11:59pm\n", "\n", "**Rules**:\n", "\n", "1. All homework must be done individually, but you are encouraged to post questions on Piazza\n", "\n", "2. No late homework will be accepted (unless you use your late-day tokens)\n", "\n", "3. Submit your code on [autograder.io](http://autograder.io/)\n", "\n", "4. Remember that copying-and-pasting code from other sources is not allowed\n", "\n", "5. The use of additional package imports beyound the packages we provide is not allowed. The autograder will not grade your code if you use additional packages.\n", "\n", "**Instructions**\n", "- Each problem will give you a file with some template code, and you need to fill in the\n", "rest.\n", "- We use the autograder, so if you\u2019re ever wondering \u201cwill I get full points for\n", "this?\u201d just upload your code in the autograder to check. There is no limit to how\n", "many times you can upload to autograder.\n", "- The autograder may test your problem with multiple different inputs to make sure it is correct.\n", "- The autograder will only show you if you got it right/wrong, so if you don\u2019t get full points, try to test with some other inputs."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Planar Pushing Learning (100 points)\n", "For this assignment we will train a robot to push an object to a goal position.\n", "To this end, you will have to collect dynamics data, process the data, train a model of the dynamics, and use it for planning a sequence of pushes to reach the target location.\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Simulation Environment Introduction\n", "\n", "For our planar pushing setup, we will use PyBullet to simulate a panda robot that pushes a block on top of a table.\n", "The pushing block is visualized in white. In green, we visualize the goal pose of the block.\n", "\n"], "outputs": []}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["from panda_pushing_env import PandaPushingEnv\n", "from visualizers import GIFVisualizer, NotebookVisualizer"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the following code block to visualize the robot pushing the block."], "outputs": []}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "\n", "# Create the visualizer\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "# Initialize the simulation environment\n", "env = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=True,  camera_heigh=800, camera_width=800)\n", "env.reset()\n", "\n", "# Perform a sequence of 3 random actions:\n", "for i in tqdm(range(3)):\n", "    action_i = env.action_space.sample()\n", "    state, reward, done, info = env.step(action_i)\n", "    if done:\n", "        break\n", "\n", "plt.close(fig)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For speeding up the visualization, we will only visualize the pushing part of the robot motion. However, be aware of the full robot motion."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["There is another option for visualization, which is to create a `.gif` file. The following cell shows an example of this. Note that here we set the rendering mode to just render the push motion for efficiency."], "outputs": []}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# Create the GIF visualizer\n", "visualizer = GIFVisualizer()\n", "\n", "# Initialize the simulation environment. This will only render push motions, omitting the robot reseting motions.\n", "env = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=False, camera_heigh=500, camera_width=500, render_every_n_steps=5)\n", "env.reset()\n", "\n", "# Perform a sequence of 3 random actions:\n", "for i in tqdm(range(10)):\n", "    action_i = env.action_space.sample()\n", "    state, reward, done, info = env.step(action_i)\n", "    if done:\n", "        break\n", "\n", "# Create and visualize the gif file.\n", "Image(filename=visualizer.get_gif())\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Rendering is useful for visualizing the implementation's performance and debugging. However, it consumes resources and takes time to produce. The following cell will perform 100 pushes without rendering. Compare the time with the previous rendering approaches. This should be an order of magnitude faster."], "outputs": []}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["# Simulate pushing without rendering - this should take about 30s.\n", "visualizer = None # No redering\n", "\n", "env = PandaPushingEnv(visualizer=visualizer)\n", "env.reset()\n", "\n", "pbar =  tqdm(range(100)) # We will perform 100 pushes.\n", "\n", "for i in pbar:\n", "    action_i = env.action_space.sample()\n", "    state, reward, done, info = env.step(action_i)\n", "    if done:\n", "        env.reset()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### State Space and Action Space\n", "\n", "For the planar pushing task, we have the following action and state spaces:\n", "\n", "The image below illustrates the state space, which is the block planar location on top of the table. \n", "$$\n", "\\mathbf x = \\begin{bmatrix} x & y & \\theta\\end{bmatrix}^\\top\\in \\text{SE}(2)\n", "$$\n", "Note that the state contains only position elements, and not velocity terms. This is because we assume that the pushing task is *quasi-static*. This means that the pushing actions are slow enough that the velocity and inertia of the block are negligible. In other words, if the robot stops pushing, the block also stops.\n", "\n", "\n", "![State Space](https://drive.google.com/uc?export=view&id=1iuHwUC_IVsrBbgbyiR8qXDV4OBmgKObU)\n", "\n", "Note that the robot is centered at the origin. \n", "\n", "\n", "The following image show the robot action space. \n", "\n", "![Action Space](https://drive.google.com/uc?export=view&id=1sFyNOAj-RmNbwk_Ww8VxmmkCaW4LpVZj)\n", "\n", "Each action $\\mathbf u = \\begin{bmatrix} p & \\phi & \\ell\\end{bmatrix}^\\top\\in \\mathbb R^3$ is composed by:\n", "* $p \\in [-1,1]$: pushing location along the lower block edge.\n", "* $\\phi \\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]$ pushing angle.\n", "* $\\ell\\in [0,1]$ pushing length as a fraction of the maximum pushing length. The maximum pushing length is is 0.1 m\n", "\n", "\n", "\n", "### Gym Environments\n", "\n", "Our planar pushing task has been wrapped into a `gym` environment. You can find more information about the gym environments [here](https://gymnasium.farama.org/api/env/).\n", "\n", "As a gym enviroment, our pushing enviroment has the following useful methods:\n", "\n", "* `step`: Given an action vector, it perform the action in the simulator and returns \n", "    1. `state`: State resultant of the action application, i.e. $s_{t+1}$\n", "    2. `reward`: Not used here (useful for Reinforcement Learning tasks).\n", "    3. `done`: Boolean. In our case the simulation is done (`done=True`) if the robot has reache the goal location or if the block has left the space limits.\n", "    4. `info`: Dictionary containing additional data. Not used here.\n", "Example:\n", "```python\n", "state, reward, done, info = env.step(action_i)\n", "```\n", "* `reset`: Resets the simulation to the initial state. It returns the inital state after reset.\n", "Example:\n", "```python\n", "    state = env.reset()\n", "```\n", "\n", "Moreover, our pushing environment has the following atributes:\n", "* `env.action_space`: Represents the action space, following the described parametrization above.\n", "* `env.state_space`: Represents the state space (block pose in $SE(2)$)\n", "These are `gym.spaces` and therefore you can sample them using `.sample()`.\n", "Example:\n", "```python\n", "action_i = env.action_space.sample()\n", "```\n", "This produces actions uniformly sampled from the action space.\n", "\n", "You can find more info about the spaces [here](https://gymnasium.farama.org/api/spaces/).\n", "\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1 - Collect Data  (15 points)\n", "\n", "First of all, given the pushing environment you will have to collect data. The collected data will be used to train a model of the pushing dynamics.\n", "\n", "**TODO:**\n", "\n", "* Implement `collect_data_random` in `learning_state_dynamics.py` which collects data trajectories. The actions should be uniformly random sampled within the action space limits.\n", "\n", "\n", "**OBSERVATION**: Data collection may take some time. Please be patient and do not close the session while you are collecting data or the data collection process may be interrupted. Once the data is collected it will be saved, so if you restart the kernel you will not need to recollect the data.\n", "\n", "\n", "**GRADING**: For grading you will also have to submit the saved collected data file `collected_data.npy`. We will check that the collected data is in the required format. However, we will NOT check the quality of the data. Note that for training a good model, data quality is key. Your collected data should be rich and diverse.\n"], "outputs": []}, {"cell_type": "code", "execution_count": 98, "metadata": {}, "outputs": [], "source": ["# Collect data (it may take some time)\n", "from learning_state_dynamics import collect_data_random\n", "\n", "# Data collection parameters\n", "N = 100 # Number of trajectories\n", "T = 10 # Trajectory length\n", "\n", "# Initialize the environment and collect data\n", "env = PandaPushingEnv()\n", "env.reset()\n", "collected_data = collect_data_random(env, num_trajectories=N, trajectory_length=T)\n", "\n", "\n", "# Verify the number of data collected:\n", "print(f'We have collected {len(collected_data)} trajectories')\n", "print('A data sample contains: ')\n", "for k, v in collected_data[0].items():\n", "    assert(type(v) == np.ndarray)\n", "    assert(v.dtype == np.float32)\n", "    print(f'\\t {k}: numpy array of shape {v.shape}')\n", "\n", "# Save the collected data into a file\n", "np.save(os.path.join(GOOGLE_DRIVE_PATH, 'collected_data.npy'), collected_data)\n"]}, {"cell_type": "code", "execution_count": 99, "metadata": {}, "outputs": [], "source": ["# Load the collected data: \n", "collected_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'collected_data.npy'), allow_pickle=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2 - Data Processing (20 points)\n", "Next, given the collected data you will have to process it so we can use it for training the model.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.1 - Single Step Data (10 points)\n", "\n", "Here we will process the data in a single-step format. \n", "\n", "\n", "**TODO:**\n", "\n", "* Implement `process_data_single_step` in `learning_state_dyanmics.py` to pack the collected data into a Dataset and returns a DataLoader for training and for validation.\n", "You may need to do the following steps:\n", " 1. Implement `SingleStepDynamicsDataset` which wraps the collected data into a PyTorch Dataset.\n", "    * For this part you only need to implement the `__getitem__` method which returns the data $(\\mathbf x_t, \\mathbf u_t, \\mathbf x_{t+1})$ as a dictionary\n", "    * Your dataset should consist of all $(\\mathbf x_t, \\mathbf u_t, \\mathbf x_{t+1})$ in the collected data\n", " 2. You should instantiate your `SingleStepDynamicsDataset` using the entire dataset, and use `torch.utils.data.random_split` to split the dataset into train and validation sets.  https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split. You should do a 80%-20% split.\n", " 3. You should return a DataLoader for the training and validation set which loads batched of the data of the required `batch_size`."], "outputs": []}, {"cell_type": "code", "execution_count": 103, "metadata": {}, "outputs": [], "source": ["# Process the data\n", "from learning_state_dynamics import process_data_single_step, SingleStepDynamicsDataset\n", "\n", "batch_size = 500\n", "train_loader, val_loader = process_data_single_step(collected_data, batch_size=batch_size)\n", "\n", "# let's check your dataloader\n", "\n", "# you should return a dataloader\n", "print(isinstance(train_loader, torch.utils.data.DataLoader))\n", "\n", "# You should have used random split to split your data - \n", "# this means the validation and training sets are both subsets of an original dataset\n", "print(isinstance(train_loader.dataset, torch.utils.data.Subset))\n", "\n", "# The original dataset should be of a SingleStepDynamicsDataset\n", "print(isinstance(train_loader.dataset.dataset, SingleStepDynamicsDataset))\n", "\n", "# we should see the state, action and next state of shape (batch_size, 3)\n", "for item in train_loader:\n", "    print(item['state'].shape)\n", "    print(item['action'].shape)\n", "    print(item['next_state'].shape)\n", "    break\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2.2 - Multi-step Data  (10 points)\n", "\n", "Here we will process the data in a multi-step format. \n", "\n", "\n", "**TODO:**\n", "\n", "* Implement `process_data_multiple_step` in `learning_state_dyanmics.py` to pack the collected data into a Dataset and returns a DataLoader for training and for validation.\n", "You may need to do the following steps:\n", " 1. Implement `MultiStepDynamicsDataset` which wraps the collected data into a PyTorch Dataset.\n", "    * For this part you only need to implement the `__getitem__` method which returns $(\\mathbf x_t, \\mathbf u_t, ..., \\mathbf u_{t+k} \\mathbf x_{t+1}, ... , \\mathbf x_{t+k+1})$, where $k$ is `num_steps`. We will be using `num_steps=4`. \n", "    * Your dataset should consist of all subtrajectories of the above form in the dataset. For instance, the trajectory $[(\\mathbf x_1,\\mathbf  u_1), (\\mathbf x_2, \\mathbf u_2), (\\mathbf x_3, \\mathbf u_3), (\\mathbf x_4, \\mathbf u_4), (\\mathbf x_5, \\mathbf u_5), (\\mathbf x_6, \\mathbf u_6), (\\mathbf x_7)]$ should result in the following items in the dataset:\n", "        - $(\\mathbf x_1, \\mathbf u_1, \\mathbf u_2, \\mathbf u_3, \\mathbf u_4, \\mathbf x_2, \\mathbf x_3, \\mathbf x_4, \\mathbf x_5)$\n", "        - $(\\mathbf x_2, \\mathbf u_2, \\mathbf u_3, \\mathbf u_4, \\mathbf u_5, \\mathbf x_3, \\mathbf x_4, \\mathbf x_5, \\mathbf x_6)$\n", "        - $(\\mathbf x_3, \\mathbf u_3, \\mathbf u_4, \\mathbf u_5, \\mathbf u_6, \\mathbf x_4, \\mathbf x_5, \\mathbf x_6, \\mathbf x_7)$\n", " 2. You should instantiate your `SingleStepDynamicsDataset` using the entire dataset, and use `torch.utils.data.random_split` to split the dataset into train and validation sets  https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split. You should do a 80%-20% split.\n", " 3. You should return a DataLoader for the training and validation set which loads batched of the data of the required `batch_size`."], "outputs": []}, {"cell_type": "code", "execution_count": 104, "metadata": {}, "outputs": [], "source": ["# Process the data\n", "from learning_state_dynamics import process_data_multiple_step, MultiStepDynamicsDataset\n", "\n", "\n", "train_loader, val_loader = process_data_multiple_step(collected_data, batch_size=500)\n", "\n", "# TODO: Add code so students can check that they have the right implementation.\n", "# you should return a dataloader\n", "print(isinstance(train_loader, torch.utils.data.DataLoader))\n", "\n", "# You should have used random split to split your data - \n", "# this means the validation and training sets are both subsets of an original dataset\n", "print(isinstance(train_loader.dataset, torch.utils.data.Subset))\n", "\n", "# The original dataset should be of a SingleStepDynamicsDataset\n", "print(isinstance(train_loader.dataset.dataset, MultiStepDynamicsDataset))\n", "\n", "# we should see the state is a tensor of shape (batch_size, 3)\n", "# action and next state should be tensors of shape (batch_size, num_steps, 3)\n", "for item in train_loader:\n", "    print(item['state'].shape)\n", "    print(item['action'].shape)\n", "    print(item['next_state'].shape)\n", "    break"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3 - Learn the Pushing Dynamics (35 points)\n", "In this part, we will train a Neural Network (NN) to model the pushing dynamics. We will formulate two approaches: absolute dynamics and residual dynamics, and compare them.\n"], "outputs": []}, {"cell_type": "code", "execution_count": 105, "metadata": {}, "outputs": [], "source": ["from learning_state_dynamics import AbsoluteDynamicsModel, ResidualDynamicsModel, SE2PoseLoss, SingleStepLoss"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.1 - Absolute Dynamics Learning (20 points)\n", "Given the collected data triplets $(\\mathbf x_{t} , \\mathbf u_{t}, \\mathbf x_{t+1})$ we will learn to predict the state transition dynamics\n", "\n", "$$\\hat{\\mathbf x}_{t+1} = f(\\mathbf x_{t} , \\mathbf u_{t}) $$\n", "\n", "**TODO:**\n", "\n", "1. In `learning_state_dynamics.py` implement `AbsoluteDynamicsModel`. \n", "    * Its forward method should compute $\\hat{\\mathbf x}_{t+1}$ . Here, the network weights will compute $\\Delta \\mathbf x_{t}$, which will need to be summed to $\\mathbf x_t$ to obtain $\\hat{\\mathbf x}_{t+1}$\n", "    * The model architecture will be a 3 linear layer NN with hidden sizes of 100 and ReLU activations.\n", "2. Implement `SE2PoseLoss` in `learning_state_dynamics.py` to compute the loss of a $SE(2)$ state. \n", "    * Its forward method computes the loss between two batched sets of $SE(2)$ transformations. \n", "    * SE(2) vectors contain two position elements and one orientation element, i.e. $\\mathbf q = \\begin{bmatrix}x & y & \\theta\\end{bmatrix}^\\top \\in SE(2)$\n", "    * To combine the different dimensions of pose and orientation, here we will exploit the fact that we know the object geometry. The error in orientation can be converted to error in position using the *(radius of gyration)[https://en.wikipedia.org/wiki/Radius_of_gyration]*. Since the object is a rectangle of width $w$ and length $l$, the radius of gyration is:\n", "    $$\n", "    r_g = \\sqrt{\\frac{w^2 + l^2}{12}}\n", "    $$\n", "\n", "    Therefore, to compare two poses  $\\mathbf q_1 = \\begin{bmatrix}x_1 & y_1 & \\theta_1\\end{bmatrix}^\\top, \\mathbf q_2 = \\begin{bmatrix}x_2 & y_2 & \\theta_2\\end{bmatrix}^\\top, \\mathbf q_1, \\mathbf q_2 \\in SE(2)$ , we can do so as:\n", "\n", "    $$\n", "    \\mathcal L(\\mathbf q_1, \\mathbf q_2) = \\text{MSE}(x_1, x_2) + \\text{MSE}(y_1, y_2) + r_g \\text{MSE}(\\theta_1, \\theta_2)\n", "    $$\n", "    $$\n", "    \\\\\n", "    $$\n", "\n", "3. Implement `SingleStepLoss.forward` in `learning_state_dynamics.py`. This method will performs a prediction with the model and computes the loss using the above loss function.\n", "3. Train the model and save it as `pushing_absolute_dynamics_model.pt`. You will also have to submit this file.\n", "\n", "HINT:\n", "* You can reuse code from HW1 for training the models with slight modifications."], "outputs": []}, {"cell_type": "code", "execution_count": 106, "metadata": {}, "outputs": [], "source": ["# Train the dynamics model\n", "\n", "pushing_absolute_dynamics_model = AbsoluteDynamicsModel(3,3)\n", "\n", "train_loader, val_loader = process_data_single_step(collected_data)\n", "\n", "pose_loss = SE2PoseLoss(block_width=0.1, block_length=0.1)\n", "pose_loss = SingleStepLoss(pose_loss)\n", "\n", "# --- Your code here\n", "\n\n\n", "    # --- Your code here\n", "\n\n\n", "    # ---\n", "    for batch_idx, batch in enumerate(train_loader):\n", "        # --- Your code here\n", "\n\n\n", "        # ---\n", "        train_loss += loss.item()\n", "    return train_loss/len(train_loader)\n", "\n", "\n", "def val_step(model, val_loader) -> float:\n", "    \"\"\"\n", "    Perfoms an epoch of model performance validation\n", "    :param model: Pytorch nn.Module\n", "    :param train_loader: Pytorch DataLoader\n", "    :param optimizer: Pytorch optimizer\n", "    :return: val_loss <float> representing the average loss among the different mini-batches\n", "    \"\"\"\n", "    val_loss = 0. # TODO: Modify the value\n", "    # Initialize the validation loop\n", "    # --- Your code here\n", "\n\n\n", "    # ---\n", "    for batch_idx, batch in enumerate(val_loader):\n", "        loss = None\n", "        # --- Your code here\n", "\n\n\n", "        # ---\n", "        val_loss += loss.item()\n", "    return val_loss/len(val_loader)\n", "\n", "\n", "def train_model(model, train_dataloader, val_dataloader, num_epochs=100, lr=1e-3):\n", "    \"\"\"\n", "    Trains the given model for `num_epochs` epochs. Use SGD as an optimizer.\n", "    You may need to use `train_step` and `val_step`.\n", "    :param model: Pytorch nn.Module.\n", "    :param train_dataloader: Pytorch DataLoader with the training data.\n", "    :param val_dataloader: Pytorch DataLoader with the validation data.\n", "    :param num_epochs: <int> number of epochs to train the model.\n", "    :param lr: <float> learning rate for the weight update.\n", "    :return:\n", "    \"\"\"\n", "    optimizer = None\n", "    # Initialize the optimizer\n", "    # --- Your code here\n", "\n\n\n", "    # ---\n", "    pbar = tqdm(range(num_epochs))\n", "    train_losses = []\n", "    val_losses = []\n", "    for epoch_i in pbar:\n", "        train_loss_i = None\n", "        val_loss_i = None\n", "        # --- Your code here\n", "\n\n\n", "        # ---\n", "        pbar.set_description(f'Train Loss: {train_loss_i:.4f} | Validation Loss: {val_loss_i:.4f}')\n", "        train_losses.append(train_loss_i)\n", "        val_losses.append(val_loss_i)\n", "    return train_losses, val_losses\n", "\n", "\n", "\n", "LR = 0.001\n", "NUM_EPOCHS = 1000\n", "\n", "train_losses, val_losses = train_model(pushing_absolute_dynamics_model, train_loader, val_loader, num_epochs=NUM_EPOCHS, lr=LR)\n", "\n", "\n", "# plot train loss and test loss:\n", "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 3))\n", "axes[0].plot(train_losses)\n", "axes[0].grid()\n", "axes[0].set_title('Train Loss')\n", "axes[0].set_xlabel('Epochs')\n", "axes[0].set_ylabel('Train Loss')\n", "axes[0].set_yscale('log')\n", "axes[1].plot(val_losses)\n", "axes[1].grid()\n", "axes[1].set_title('Validation Loss')\n", "axes[1].set_xlabel('Epochs')\n", "axes[1].set_ylabel('Validation Loss')\n", "axes[1].set_yscale('log')\n", "\n", "# ---\n", "\n", "# save model:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'pushing_absolute_dynamics_model.pt')\n", "torch.save(pushing_absolute_dynamics_model.state_dict(), save_path)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.2 - Residual Dynamics Learning (10 points)\n", "Given the collected data triplets $(\\mathbf x_{t} , \\mathbf u_{t}, \\mathbf x_{t+1})$ we will learn to predict the state transition dynamics\n", "\n", "$$\\hat{\\mathbf x}_{t+1} = \\mathbf x_{t} + \\Delta \\mathbf x_{t} =  \\mathbf x_{t} + f(\\mathbf x_{t}, \\mathbf u_{t}) $$\n", "\n", "\n", "**TODO:**\n", "\n", "1. In `learning_state_dynamics.py` implement `ResidualDynamicsModel`.\n", "    * Its forward method should compute $\\hat{\\mathbf x}_{t+1}$ . Here, the network weights will compute $\\Delta \\mathbf x_{t}$, which will need to be summed to $\\mathbf x_t$ to obtain $\\hat{\\mathbf x}_{t+1}$\n", "    * The model architecture will be a 3 linear layer NN with hidden sizes of 100 and ReLU activations.\n", "2. Train the model and save it as `pushing_residual_dynamics_model.pt`. You will also have to submit this file.\n"], "outputs": []}, {"cell_type": "code", "execution_count": 107, "metadata": {}, "outputs": [], "source": ["pushing_residual_dynamics_model = None\n", "train_loader, val_loader = process_data_single_step(collected_data)\n", "\n", "pose_loss = SE2PoseLoss(block_width=0.1, block_length=0.1)\n", "pose_loss = SingleStepLoss(pose_loss)\n", "\n", "# --- Your code here\n", "\n\n\n", "# ---\n", "\n", "# save model:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'pushing_residual_dynamics_model.pt')\n", "torch.save(pushing_residual_dynamics_model.state_dict(), save_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.3 - Absolute Dynamics vs Residual Dynamics (No points)\n", "In this section we will compare the performance of the absolute dynamics model and the residual dynamics model with test data that we provide you.\n", "\n", "Which one does perform better? Why is that the case?\n"], "outputs": []}, {"cell_type": "code", "execution_count": 172, "metadata": {}, "outputs": [], "source": ["val_dataset = SingleStepDynamicsDataset(np.load('validation_data.npy', allow_pickle=True))\n", "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset))\n", "\n", "pose_loss = SE2PoseLoss(block_width=0.1, block_length=0.1)\n", "pose_loss = SingleStepLoss(pose_loss)\n", "\n", "loss_absolute = 0.0\n", "loss_residual = 0.0\n", "\n", "for item in val_loader:\n", "    loss_absolute += pose_loss(pushing_absolute_dynamics_model, item['state'], item['action'], item['next_state'])\n", "    loss_residual += pose_loss(pushing_residual_dynamics_model, item['state'], item['action'], item['next_state'])\n", "\n", "print(f'Validation loss for absolute dynamics model is {loss_absolute}')\n", "print(f'Validation loss for residual dynamics model is {loss_residual}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.4 Multi-step Loss (10 points)\n", "\n", "In this section you will train a residual dynamics model using a multistep loss as you have seen in the lectures. \n", "\n", "**TODO**:\n", "Implement `MultiStepLoss` in in `learning_state_dyanmics.py` \n", "* The parameter `discount` is the same as $\\gamma$ from the lectures. \n", "* HINT: You may need to reduce the learning rate and train for slightly longer for the multi-step loss"], "outputs": []}, {"cell_type": "code", "execution_count": 168, "metadata": {}, "outputs": [], "source": ["# Train the dynamics model\n", "from learning_state_dynamics import MultiStepLoss, process_data_multiple_step\n", "pushing_multistep_residual_dynamics_model = None\n", "train_loader, val_loader = process_data_multiple_step(collected_data, batch_size=64)\n", "\n", "pose_loss = SE2PoseLoss(block_width=0.1, block_length=0.1)\n", "pose_loss = MultiStepLoss(pose_loss, discount=0.9)\n", "\n", "# --- Your code here\n", "\n\n\n", "# ---\n", "\n", "# save model:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'pushing_multi_step_residual_dynamics_model.pt')\n", "torch.save(pushing_multistep_residual_dynamics_model.state_dict(), save_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3.5 - Multistep vs Single Step Loss (No points)\n", "In this section we will compare the performance of the multistep and single step loss for the residual dynamics model with test data that we provide you.\n", "\n", "You should see that both models should perform similarly when evalauted on the single step loss, but that the model trained on a multi-step loss has lower loss for the multi-step loss. Even when we train with a `num_steps` value of 4 and evalaute with a `num_steps` value of 10!  \n"], "outputs": []}, {"cell_type": "code", "execution_count": 170, "metadata": {}, "outputs": [], "source": ["val_dataset = SingleStepDynamicsDataset(np.load('validation_data.npy', allow_pickle=True))\n", "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=len(val_dataset))\n", "\n", "num_steps = 10\n", "val_multistep_dataset = MultiStepDynamicsDataset(np.load('validation_data.npy', allow_pickle=True), num_steps=num_steps)\n", "val_multistep_loader = torch.utils.data.DataLoader(val_multistep_dataset, batch_size=len(val_multistep_dataset))\n", "\n", "pose_loss = SE2PoseLoss(block_width=0.1, block_length=0.1)\n", "multistep_pose_loss = MultiStepLoss(pose_loss, discount=1)\n", "pose_loss = SingleStepLoss(pose_loss)\n", "\n", "\n", "single_step_loss_single_step_model = 0.0\n", "single_step_loss_multi_step_model = 0.0\n", "\n", "for item in val_loader:\n", "    single_step_loss_single_step_model += \\\n", "        pose_loss(pushing_residual_dynamics_model, item['state'], item['action'], item['next_state'])\n", "\n", "    single_step_loss_multi_step_model += \\\n", "        pose_loss(pushing_multistep_residual_dynamics_model, item['state'], item['action'], item['next_state'])\n", "\n", "print(f'Validation single-step loss for model trained on single-step loss {single_step_loss_single_step_model}')\n", "print(f'Validation single-step loss for model trained on multi-step loss {single_step_loss_multi_step_model}')\n", "\n", "multi_step_loss_single_step_model = 0.0\n", "multi_step_loss_multi_step_model = 0.0\n", "\n", "for item in val_multistep_loader:\n", "    multi_step_loss_single_step_model += \\\n", "        multistep_pose_loss(pushing_residual_dynamics_model, item['state'], item['action'], item['next_state'])\n", "\n", "    multi_step_loss_multi_step_model += \\\n", "        multistep_pose_loss(pushing_multistep_residual_dynamics_model, item['state'], item['action'], item['next_state'])\n", "print('')\n", "print(f'Validation multi-step loss for model trained on single-step loss {multi_step_loss_single_step_model / num_steps}')\n", "print(f'Validation multi-step loss for model trained on multi-step loss {multi_step_loss_multi_step_model / num_steps}')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4 - Use the Learned Dynamics Model for Planning and Controls (30 points)\n", "Finally, once we have a good model of the pushing dynamics, we can use it to plan a sequence of actions to reach the goal configuration.\n", "\n", "To do so, we will be using MPPI as our controller. It will rollout the trajectories using the learned dynamics model from the previous section. Since in HW2 you already implemented MPPI, here we are giving you the implmentation. You will just have to implement the cost function and some minor other parts. We are also giving you some good initialization for the hyperparametes. However, you may need to tune them for your model and cost function implementations.\n"], "outputs": []}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["from learning_state_dynamics import PushingController, free_pushing_cost_function, collision_detection, obstacle_avoidance_pushing_cost_function\n", "from panda_pushing_env import TARGET_POSE_FREE, TARGET_POSE_OBSTACLES, BOX_SIZE"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.1 - Obstacle Free Pushing (15 points)\n", "In this first part, we will add an obstacle to the scene.\n", "\n", "**TODO:**\n", "\n", "* Implement `free_pushing_cost_function` in `learning_state_dynamics.py`.\n", "  This function should compute the state cost for MPPI.\n", "  \n", "* Complete the MPPI-based controller classs `PushingController` in `learning_state_dynamics.py`.\n", "  You need to implement:\n", "   * Tune mppi controller hyperparameters in the `__init__` method.\n", "   * Implement the `_compute_dyanmics` method which should compute `next_state` from `state` and `action`.\n", "   * Fix the `control` method.\n", "   \n", "Read carefully all docstring for details about the expected inputs, outputs and uses.\n"], "outputs": []}, {"cell_type": "code", "execution_count": 96, "metadata": {}, "outputs": [], "source": ["# Control on an obstacle free environment\n", "%matplotlib inline\n", "\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "\n", "env = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5)\n", "controller = PushingController(env, pushing_residual_dynamics_model, free_pushing_cost_function, num_samples=100, horizon=10)\n", "env.reset()\n", "\n", "state_0 = env.reset()\n", "state = state_0\n", "\n", "# num_steps_max = 100\n", "num_steps_max = 20\n", "\n", "for i in tqdm(range(num_steps_max)):\n", "    action = controller.control(state)\n", "    state, reward, done, _ = env.step(action)\n", "    if done:\n", "        break\n", "\n", "        \n", "# Evaluate if goal is reached\n", "end_state = env.get_state()\n", "target_state = TARGET_POSE_FREE\n", "goal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) # evaluate only position, not orientation\n", "goal_reached = goal_distance < BOX_SIZE\n", "\n", "print(f'GOAL REACHED: {goal_reached}')\n", "        \n", "        \n", "plt.close(fig)\n", "\n", "\n", "\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4.2 - Pushing with Obstacles (15 points)\n", "Now, we will repeat the task but adding an obstacle to the scene. The goal is to push the block to the goal while avoiding the obstacle.\n", "You will have to first implement a basic collision detection\n", "\n", "Then, you will have to use the collision detection to tune the cost function so the controlled trajectory reaches the goal position without colliding with the obstacles.\n"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 4.2.1 - Collision Detection Function (5 points)\n", "**TODO:**\n", "* Implement `collision_detection` in `learning_state_dynamics.py`."], "outputs": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["#### 4.2.2 - Cost Function for Obstacle Avoidance (10 points)\n", "**TODO:**\n", "\n", "* Implement `obstacle_avoidance_pushing_cost_function` in `learning_state_dynamics.py`."], "outputs": []}, {"cell_type": "code", "execution_count": 97, "metadata": {}, "outputs": [], "source": ["# Control on an obstacle free environment\n", "%matplotlib inline\n", "\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "\n", "env = PandaPushingEnv(visualizer=visualizer, render_non_push_motions=False,  include_obstacle=True, camera_heigh=800, camera_width=800, render_every_n_steps=5)\n", "controller = PushingController(env, pushing_multistep_residual_dynamics_model,\n", "                               obstacle_avoidance_pushing_cost_function, num_samples=1000, horizon=20)\n", "env.reset()\n", "\n", "state_0 = env.reset()\n", "state = state_0\n", "\n", "# num_steps_max = 100\n", "num_steps_max = 20\n", "\n", "for i in tqdm(range(num_steps_max)):\n", "    action = controller.control(state)\n", "    state, reward, done, _ = env.step(action)\n", "    if done:\n", "        break\n", "\n", "        \n", "# Evaluate if goal is reached\n", "end_state = env.get_state()\n", "target_state = TARGET_POSE_OBSTACLES\n", "goal_distance = np.linalg.norm(end_state[:2]-target_state[:2]) # evaluate only position, not orientation\n", "goal_reached = goal_distance < BOX_SIZE\n", "\n", "print(f'GOAL REACHED: {goal_reached}')\n", "        \n", "        \n", "# Evaluate state\n", "plt.close(fig)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"colab": {"authorship_tag": "ABX9TyNRQ/BgPmM29eFX3MyfIYKz", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 1}