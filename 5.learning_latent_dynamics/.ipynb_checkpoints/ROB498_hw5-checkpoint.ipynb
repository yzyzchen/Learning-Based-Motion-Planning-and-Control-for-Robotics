{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# ROB 498: Robot Learning for Planning and Control\n", "# Assignment 5: Learning Latent Space Dynamics"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Colab Setup"], "outputs": []}, {"cell_type": "code", "execution_count": 1, "metadata": {"pycharm": {"is_executing": true}}, "outputs": [], "source": ["# TODO: Fill in the Google Drive path where you uploaded the assignment\n", "# Example: If you create a ROB498 folder and put all the files under HW4 folder, then 'ROB498/HW4'\n", "GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'ROB498/HW5'\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Setup Code "], "outputs": []}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": ["%load_ext autoreload\n", "%autoreload 2"]}, {"cell_type": "code", "execution_count": 23, "metadata": {}, "outputs": [], "source": ["from google.colab import drive\n", "\n", "drive.mount('/content/drive')"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["import os\n", "import sys\n", "\n", "GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n", "\n", "files = os.listdir(GOOGLE_DRIVE_PATH)\n", "expected_files = [ 'ROB498_hw5.ipynb', 'panda_pushing_env.py', 'visualizers.py', 'mppi.py', 'utils.py', 'pushing_image_data.npy', 'pushing_image_validation_data.npy']\n", "\n", "sys.path.append(GOOGLE_DRIVE_PATH)\n", "\n", "# Verify that there are all the expected files in the directory\n", "all_found = True\n", "for expected_file in expected_files:\n", "    if expected_file not in files:\n", "        print(f'Required file {expected_file} not found!')\n", "        all_found = False\n", "if all_found:\n", "    print('All required files are found :)')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Install missing required packages \n", "# Unfortunately Colab does not have pybullet package by default, so we will have to install it every time that the notebook kernel is restarted.\n", "# Install pybullet -- For simulation purposes\n", "!pip install pybullet\n", "# Install numpngw -- For visualization purposes\n", "!pip install numpngw"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["import torch\n", "import os\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "import numpy as np\n", "from torch.utils.data import Dataset, DataLoader\n", "import matplotlib.pyplot as plt\n", "from numpngw import write_apng\n", "from IPython.display import Image\n", "from tqdm.notebook import tqdm"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Assignment Introduction"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Due 4/12 at 11:59pm\n", "\n", "**Rules**:\n", "\n", "1. All homework must be done individually, but you are encouraged to post questions on Piazza\n", "\n", "2. No late homework will be accepted (unless you use your late-day tokens)\n", "\n", "3. Submit your code on [autograder.io](http://autograder.io/)\n", "\n", "4. Remember that copying-and-pasting code from other sources is not allowed\n", "\n", "5. The use of additional package imports beyound the packages we provide is not allowed. The autograder will not grade your code if you use additional packages.\n", "\n", "**Instructions**\n", "- Each problem will give you a file with some template code, and you need to fill in the\n", "rest.\n", "- We use the autograder, so if you\u2019re ever wondering \u201cwill I get full points for\n", "this?\u201d just upload your code in the autograder to check. There is no limit to how\n", "many times you can upload to autograder.\n", "- The autograder may test your problem with multiple different inputs to make sure it is correct.\n", "- The autograder will only show you if you got it right/wrong, so if you don\u2019t get full points, try to test with some other inputs."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Planar Pushing Learning From Images (100 points)\n", "\n", "In this assignment we are again considering the planar pushing environment you have seen in HW3 and HW4. \n", "\n", "However, for this homework, we we will learn the dynamics directly from images. \n", "\n", "### State Space and Action Space\n", "\n", "For the planar pushing task, we use the same action space as in HW3. The following image show the robot action space. \n", "\n", "![Action Space](https://drive.google.com/uc?export=view&id=1sFyNOAj-RmNbwk_Ww8VxmmkCaW4LpVZj)\n", "\n", "Each action $\\mathbf u = \\begin{bmatrix} p & \\phi & \\ell\\end{bmatrix}^\\top\\in \\mathbb R^3$ is composed by:\n", "* $p \\in [-1,1]$: pushing location along the lower block edge.\n", "* $\\phi \\in [-\\frac{\\pi}{2},\\frac{\\pi}{2}]$ pushing angle.\n", "* $\\ell\\in [0,1]$ pushing length as a fraction of the maximum pushing length. The maximum pushing length is is 0.1 m\n", "\n", "However, the state space is now a $32 \\times 32 $ image from an overhead camera.\n", "\n", "### Gym Environments\n", "\n", "Our planar pushing task has been wrapped into a `gym` environment. You can find more information about the gym environments [here](https://gymnasium.farama.org/api/env/).\n", "\n", "As a gym enviroment, our pushing enviroment has the following useful methods:\n", "\n", "* `step`: Given an action vector, it performs the action in the simulator and returns \n", "    1. `state`: The resulting state, i.e. $x_{t+1}$. This will be an image\n", "    2. `reward`: Not used here (useful for Reinforcement Learning tasks).\n", "    3. `done`: Boolean. In our case the simulation is done (`done=True`) if the robot has reached the goal location or if the block has left the bounds of the workspace.\n", "    4. `info`: Dictionary containing additional data. Not used here.\n", "Example:\n", "```python\n", "state, reward, done, info = env.step(action_i)\n", "```\n", "* `reset`: Resets the simulation to the initial state. It returns the inital state after reset. You can optionally reset the block to a known start pose\n", "Example:\n", "```python\n", "    state = env.reset(object_pose=start_pose)\n", "```\n", "\n", "\n", "Moreover, our pushing environment has the following attributes:\n", "* `env.action_space`: Represents the action space, following the described parametrization above.\n", "* `env.observation_space`: Represents the state space. In this case this will be the space of valid $32 \\times 32$ images. \n", "    * If `grayscale=True`, then observation space size is $(32, 32, 1)$\n", "    * If `grayscale=False`, then observation space size is $(32, 32, 3)$\n", "* `env.object_pose_space`: Represents valid object poses.\n", "These are `gym.spaces` and therefore you can sample them using `.sample()`.\n", "Example:\n", "```python\n", "action_i = env.action_space.sample()\n", "```\n", "This produces actions uniformly sampled from the action space.\n", "\n", "You can find more info about gym spaces [here](https://gymnasium.farama.org/api/spaces/).\n"], "outputs": []}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [], "source": ["from panda_pushing_env import PandaImageSpacePushingEnv\n", "from visualizers import GIFVisualizer, NotebookVisualizer\n", "\n", "from learning_latent_dynamics import *\n", "from utils import *"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You can run the below block to see the environment in action, and the corresponding image observations"], "outputs": []}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "\n", "# Create the visualizer\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "# Initialize the simulation environment\n", "env = PandaImageSpacePushingEnv(visualizer=visualizer, \n", "                                render_non_push_motions=True,  \n", "                                camera_heigh=800, \n", "                                camera_width=800, \n", "                                done_at_goal=False)\n", "env.reset()\n", "# Perform a sequence of 3 random actions:\n", "states = []\n", "for i in tqdm(range(3)):\n", "    action_i = env.action_space.sample()\n", "    state, reward, done, info = env.step(action_i)\n", "    states.append(state)\n", "    if done:\n", "        break\n", "    \n", "view_states(states)\n", "\n", "plt.close(fig)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As in the previous homeworks, we can turn off rendering in the notebook which will increase the simulation speed. The speed-up will not be as significant as in the previous homeworks, since the simulator still needs to render the camera images for the image observations.  "], "outputs": []}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Simulate pushing without rendering - this should take about 5s.\n", "visualizer = None # No redering\n", "\n", "env = PandaImageSpacePushingEnv(visualizer=visualizer)\n", "env.reset()\n", "\n", "\n", "states = []\n", "pbar =  tqdm(range(10)) # We will perform 100 pushes.\n", "\n", "for i in pbar:\n", "    action_i = env.action_space.sample()\n", "    state, reward, done, info = env.step(action_i)\n", "    states.append(state)\n", "    if done:\n", "        env.reset()\n", "        \n", "view_states(states)\n", "\n", "print(f'State shape: {states[0].shape}  (width, height, num_channels)')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Color images are information rich. However, they are harder to process and they require more memory to be stored.\n", "In this assigment we will work with grayscale image states.\n", "\n", "As opposed to color images which have 3 chanels (R,G,B), grayscale images only have one channel (*illuminance*)\n", "\n", "We can do so by setting `grayscale=True` in our custom panda environment `PandaImageSpacePushingEnv`."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1 - Collect Data (10 points):\n", "Given the environment, the first step is to collect data. In this first section, you will have to collect data. Since the data collection requires rendering images, generating a decent size dataset may take some time. To avoid unnecessary waiting time, we require you to just collect a small subset of data. The data you collected will be saved into `collected_data.npy` which you will have to submit to *Autograder*. To train the dynamics model of the pushign task, you will use the data we have collected for you which can be found in `pushing_image_data.npy`.\n", "\n", "**TODO:**\n", "\n", "* Implement `collect_data_random_trajectory` in `learning_latent_dynamics.py` which collects data trajectories. The actions should be uniformly random sampled within the action space limits.\n", "* Your method should randomly initialize the object start pose\n", "* Your method should use the `done` output of `env.step` to ensure that the block pose is valid for the entirety of all trajectories. \n", "\n", "\n", "**GRADING**: For grading you will also have to submit the saved collected data file `collected_data.npy`. We will check that the collected data is in the required format. Note that for training a good model, data quality is key. Your collected data should be diverse to train an effective model of pushing dynamics."], "outputs": []}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# Simulate pushing without rendering - this should take about 5s.\n", "visualizer = None # No redering\n", "\n", "env = PandaImageSpacePushingEnv(visualizer=visualizer, grayscale=True)\n", "env.reset()\n", "\n", "\n", "states = []\n", "pbar =  tqdm(range(10)) # We will perform 100 pushes.\n", "\n", "for i in pbar:\n", "    action_i = env.action_space.sample()\n", "    state, reward, done, info = env.step(action_i)\n", "    states.append(state)\n", "    if done:\n", "        env.reset()\n", "    \n", "print(states[0].shape)\n", "view_states(states)\n", "\n", "print(f'State shape: {states[0].shape}  (width, height, num_channels)')"]}, {"cell_type": "code", "execution_count": 10, "metadata": {}, "outputs": [], "source": ["# Collect data (it may take some time)\n", "from learning_latent_dynamics import collect_data_random_trajectory\n", "\n", "# Data collection parameters\n", "N = 15 # Number of trajectories to be collected\n", "T = 10 # Trajectory length\n", "\n", "grayscale = True\n", "\n", "# Initialize the environment and collect data\n", "env = PandaImageSpacePushingEnv(grayscale=grayscale, done_at_goal=False)\n", "env.reset()\n", "collected_data = collect_data_random_trajectory(env, num_trajectories=N, trajectory_length=T)\n", "\n", "\n", "# Verify the number of data collected:\n", "print(f'We have collected {len(collected_data)} trajectories')\n", "print('A data sample contains: ')\n", "desired_types = {'states': np.uint8, 'actions': np.float32}\n", "for k, v in collected_data[0].items():\n", "    assert(type(v) == np.ndarray)\n", "    assert(v.dtype == desired_types[k])\n", "    print(f'\\t {k}: numpy array of shape {v.shape} and type {v.dtype}')\n", "\n", "# Save the collected data into a file\n", "np.save(os.path.join(GOOGLE_DRIVE_PATH, 'collected_data.npy'), collected_data)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**LOAD THE GIVEN DATA:** The following block cell will load the data we have collected for you which you can use for training."], "outputs": []}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["# TODO: Delete this cell\n", "collected_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'pushing_image_data.npy'), allow_pickle=True)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2 - Process Data (10 points):"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Now you have a dataset, the task is to normalize the data and implement a dataloader which we will use for training. \n", "\n", "**TODO:** Implement:\n", "* `NormalizationTransform`: This is a transform class which will be used to normalize the image data with `NormalizationTransform.normalize_state` and will denormalize the data with `NormalizationTransform.denormalize_sate`. The transform will perform this normalization based on a given mean and standard deviation.  \n", "* `MultiStepDynamicsDataset`: This wraps the collected data into a PyTorch Dataset. For this part you only need to implement the `__getitem__` method which returns $(\\mathbf u_t, ..., \\mathbf u_{t+k-1}, \\mathbf{x_t} \\mathbf x_{t+1}, ... , \\mathbf x_{t+k})$, where $k$ is `num_steps`. The `__get_item__` method should return a dictionary with keys `states` and `actions`. Your `__get_item__` should also normalize the states using the class `self.transform` which will be a `NormalizationTransform`. \n", "* `process_data_multiple_step`: This function should return `train_loader`, `val_loader` and `norm_constants`. `norm_constants` should be a dictionary containing the mean and standard deviation of the training images. You should instantiate your `MultiStepDynamicsDataset` using the entire dataset, and use `torch.utils.data.random_split` to split the dataset into train and validation sets  https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split. You should do a 80%-20% split.You should return a DataLoader for the training and validation set which loads batched of the data of the required `batch_size`.\n"], "outputs": []}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": ["# Process the data\n", "from learning_latent_dynamics import process_data_multiple_step, MultiStepDynamicsDataset\n", "\n", "batch_size = 128\n", "num_steps = 4 # You may want to try different values for this and verify that your implementation works fine.\n", "train_loader, val_loader, norm_constants = process_data_multiple_step(collected_data, batch_size=batch_size, num_steps=num_steps)\n", "\n", "# let's check your dataloader\n", "\n", "# you should return a dataloader\n", "print('Is the returned train_loader a DataLoader?')\n", "print('Yes' if isinstance(train_loader, torch.utils.data.DataLoader) else 'No')\n", "print('')\n", "\n", "# You should have used random split to split your data - \n", "# this means the validation and training sets are both subsets of an original dataset\n", "print('Was random_split used to split the data?')\n", "print('Yes' if isinstance(train_loader.dataset, torch.utils.data.Subset) else 'No')\n", "print('')\n", "\n", "# The original dataset should be of a SingleStepDynamicsDataset\n", "print('Is the dataset a SingleStepDynamicsDataset?')\n", "print('Yes' if isinstance(train_loader.dataset.dataset, MultiStepDynamicsDataset) else 'No')\n", "print('')\n", "\n", "# we should see the state, action and next state of shape (batch_size, 3)\n", "for item in train_loader:\n", "    print(f'state is shape {item[\"states\"].shape}')\n", "    print(f'action is shape {item[\"actions\"].shape}')\n", "    break\n", "\n", "# save normalization constants:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'norm_constants.pt')\n", "torch.save(norm_constants, save_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**NOTE**: You will have to submit the saved normalization constants to Autograder."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3 - Variational Autoencoder  (20 points):"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["In this section, you will implement a Variational Autoencoder (VAE) and train it on the images of the environment. We will not be considering dynamics yet. \n", "\n", "First, you will implement the VAE Encoder, which maps images to a Gaussian distribution over latent vectors. The encoder outputs $\\mu$ and $\\log\\sigma^2$ which parameterize the latent distribution.The architecture is shown below:\n", "![var_state_encoder_architecture](https://drive.google.com/uc?export=view&id=15zrzkwHKect2o1Kw1603zEkGcGJKC0Cq)\n", "\n", "For the `Conv2d` operations, will have to find the kernel size that results in the desired shape.\n", "To compute this, you need to consider the following:\n", "\n", "![state_encoder_architecture](https://drive.google.com/uc?export=view&id=1dqY_NTTyoRiQ9nOiPLlEALvIXJLvMBZo)\n", "\n", "where in our case `padding=0`, `stride=1`, and `dilation=1`.\n", "For more details check [PyTorch Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n", "\n", "The `MaxPool2d` operations need to extract the maximum value for every $2\\times2$ group of pixels. You will have to specify the `kernel_size` and the `stride` to obtain the desired operation (`padding=0`).\n", "For more details check [PyTorch MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d).\n", "\n", "\n", "Finally, for mapping latent samples to images, we will use a fully-connected decoder. The required architecture is as follows:\n", "![state_decoder_architecture](https://drive.google.com/uc?export=view&id=1Hvi5fSHaLr8liA5K-eSZuWHrvBmPb5wk)\n", "\n", "\n", "**TODO:**\n", "* Implement `StateVariationalEncoder` in `learning_latent_dynamics.py`\n", "    * Need to implement the methods `__init__`, `forward` and `reparametrize`.\n", "* Implement `StateDecoder` in `learning_latent_dynamics.py`\n", "    * Need to implement the methods `__init__` and `forward`.\n", "* Implement `StateVAE` in `learning_latent_dynamics.py`\n", "    * Need to implement the methods `forward`, `encode`, and `decode`.\n", "* Implement `VAELoss` in `learning_latent_dynamics.py`\n", "    * Need to implement the `forward`."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.1 - Overfit to One Image (Memorize)  (No points)\n", "\n", "A good debuging method is to overfit to a single input image. The model should just memorize that input almost perfectly. If it does not, something must be wrong."], "outputs": []}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["# Train the dynamics model\n", "LATENT_DIM = 16\n", "NUM_CHANNELS = 1\n", "BETA = 0.001\n", "LR = 0.0001\n", "NUM_EPOCHS = 1500\n", "\n", "vae_model = StateVAE(latent_dim=LATENT_DIM, num_channels=NUM_CHANNELS)\n", "loss_func = VAELoss(beta=BETA)\n", "optimizer = optim.Adam(vae_model.parameters(), lr=LR)\n", "\n", "train_loader, val_loader, normalization_constants = process_data_multiple_step(collected_data[0:1], num_steps=1) # Single step data\n", "\n", "\n", "\n", "pbar = tqdm(range(NUM_EPOCHS))\n", "train_losses = []\n", "for epoch_i in pbar:\n", "    train_loss_i = 0.\n", "    # TODO: You should implement the training step. You need to call the encoder and the decoder a\n", "    # --- Your code here\n", "\n\n\n", "    # ---\n", "    pbar.set_description(f'Train Loss: {train_loss_i:.4f}')\n", "    train_losses.append(train_loss_i)\n", "\n", "# plot train loss and test loss:\n", "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n", "axes = [axes]\n", "axes[0].plot(train_losses)\n", "axes[0].grid()\n", "axes[0].set_title('Train Loss')\n", "axes[0].set_xlabel('Epochs')\n", "axes[0].set_ylabel('Train Loss')\n", "axes[0].set_yscale('log')\n", "\n"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# Visualize the state and its reconstruction:\n", "vae_model.eval()\n", "batches = [b for b in val_loader]\n", "norm_tr = NormalizationTransform(norm_constants)\n", "state = batches[0]['states'][0,0]\n", "z, log_var = vae_model.encoder(state)\n", "state_rec = vae_model.decoder(z)\n", "state_img = norm_tr.denormalize_state(state).detach().cpu().numpy().transpose(1,2,0).astype(np.uint8)\n", "state_rec_img = norm_tr.denormalize_state(state_rec).detach().cpu().numpy().transpose(1,2,0).astype(np.uint8)\n", "\n", "fig, axes = plt.subplots(1,2)\n", "axes[0].imshow(state_img, cmap='gray')\n", "axes[0].set_title('Input State')\n", "axes[1].imshow(state_rec_img, cmap='gray')\n", "axes[1].set_title('Reconstructed State')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If everything is fine, you should obtain a perfect reconstruction since the network has been able to memorize the single state."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3.2 - VAE Training  (20 points)\n", "\n", "Next, once we know that our VAE implementation is working, you will have to train it on the full set of data."], "outputs": []}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [], "source": ["# Train the VAE\n", "\n", "LR = 0.0001\n", "NUM_EPOCHS = 2000\n", "BETA = 0.001\n", "LATENT_DIM = 10\n", "NUM_CHANNELS = 1\n", "NUM_STEPS = 1\n", "LATENT_DIM = 16\n", "\n", "train_loader, val_loader, norm_constants = process_data_multiple_step(collected_data, num_steps=1)\n", "norm_tr = NormalizationTransform(norm_constants)\n", "\n", "all_states = []\n", "for batch_i in train_loader:\n", "    state_i = batch_i['states']\n", "    all_states.append(state_i)\n", "all_states = torch.cat(all_states, axis=0)\n", "\n", "states = all_states.flatten(end_dim=1) # Get initial states (Num satates, num_channels, w, h)\n", "\n", "\n", "vae_model = StateVAE(latent_dim=LATENT_DIM, num_channels=1)\n", "loss_func = VAELoss(beta=BETA)\n", "optimizer = optim.Adam(vae_model.parameters(), lr=LR)\n", "pbar = tqdm(range(NUM_EPOCHS))\n", "train_losses = []\n", "for epoch_i in pbar:\n", "    train_loss_i = 0.\n", "    # --- Your code here\n", "\n\n\n", "    # ---\n", "    train_loss_i += loss.item()\n", "    pbar.set_description(f'Latent dim {LATENT_DIM} - Loss: {train_loss_i:.4f}')\n", "    train_losses.append(train_loss_i)\n", "\n", "losses = train_losses\n", "vaes = vae_model\n", "# Evaluate:\n", "vae_model.eval()\n", "states_rec, mu, log_var, latent_state = vae_model(states)\n", "\n", "\n", "# plot train loss and test loss:\n", "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(12, 3))\n", "axes = [axes]\n", "axes[0].plot(losses, label=f'latent_dim: {LATENT_DIM}')\n", "axes[0].grid()\n", "axes[0].legend()\n", "axes[0].set_title('Train Loss')\n", "axes[0].set_xlabel('Epochs')\n", "axes[0].set_ylabel('Train Loss')\n", "axes[0].set_yscale('log')\n", "\n", "\n", "# ---\n", "\n", "# save model:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'vae_model.pt')\n", "torch.save(vae_model.state_dict(), save_path)"]}, {"cell_type": "code", "execution_count": 9, "metadata": {}, "outputs": [], "source": ["# Visualize the state and its reconstruction:\n", "num_states_to_plot = 10\n", "fig, axes = plt.subplots(2, num_states_to_plot, sharex=True, sharey=True, figsize=(2*num_states_to_plot, 4))\n", "axes = np.transpose(axes)\n", "for s_i in range(num_states_to_plot):\n", "    state_i = states[s_i]\n", "    state_img = norm_tr.denormalize_state(state_i).detach().cpu().numpy().transpose(1,2,0).astype(np.uint8)\n", "    axes[s_i][0].imshow(state_img, cmap='gray')\n", "    axes[s_i][0].set_title('Input State')\n", "    state_rec_i = states_rec[s_i]\n", "    state_rec_img = norm_tr.denormalize_state(state_rec_i).detach().cpu().numpy().transpose(1,2,0).astype(np.uint8)\n", "    axes[s_i][1].imshow(state_rec_img, cmap='gray')\n", "    axes[s_i][1].set_title(f'Reconstruction')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Test performance"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["**GRADING GUIDE**: We will evaluate your vae model performance on the `pushing_image_validation_data`. You should obtain at least a loss score of 0.2 or lower."], "outputs": []}, {"cell_type": "code", "execution_count": 11, "metadata": {}, "outputs": [], "source": ["test_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'pushing_image_validation_data.npy'), allow_pickle=True)\n", "norm_tr = NormalizationTransform(norm_constants)\n", "\n", "single_step_dataset = MultiStepDynamicsDataset(test_data, num_steps=1, transform=norm_tr)\n", "single_step_loader = DataLoader(single_step_dataset, batch_size=len(single_step_dataset))\n", "\n", "loss_func = VAELoss(beta=BETA)\n", "\n", "def test_vae(model, val_loader, loss_fn) -> float:\n", "    \"\"\"\n", "    Perfoms an epoch of model performance validation\n", "    :param model: Pytorch nn.Module\n", "    :param train_loader: Pytorch DataLoader\n", "    :param loss: Loss function\n", "    :return: val_loss <float> representing the average loss among the different mini-batches\n", "    \"\"\"\n", "    val_loss = 0. # TODO: Modify the value\n", "    # Initialize the validation loop\n", "    model.eval()\n", "    for batch in val_loader:\n", "        loss = None\n", "        states = batch['states'].flatten(end_dim=1)\n", "        states_rec, mu, log_var, latent_state = vae_model.forward(states)\n", "        loss = loss_func(states_rec, states, mu, log_var)\n", "        val_loss += loss.item()\n", "    return val_loss/len(val_loader)\n", "\n", "\n", "print(f'VAE model VALIDATION loss: {test_vae(vae_model, single_step_loader, loss_func)}')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4 -  Learn Latent Dynamics  (30 points)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Previously you implemented a VAE to reconstruct the images. In this section you will have to implement and train a dynamics model from images.\n", "\n", "You will implemented an Autoencoder and learn dynamics in the latent space of that autoencoder. \n", "### Architecture\n", "\n", "As you have seen in class, there exist more sophisticated approaches such as Embed to Control (E2C) which use VAEs to obtain the latent vectors. E2C imposes more structure on the latent which makes it . However, E2C has extra complexity, requires larger data samples, and take more time to train. Hence, for simplification here we will simplify the model to an encoder-decoder.\n", "\n", "The full architecture is shown below:\n", "\n", "![latent_dynamics_architecture](https://drive.google.com/uc?export=view&id=1cfXQvphGVWxeygowyXMtI63TkGEcCepN)\n", "\n", "The model components are as follows:\n", "\n", "First, you will have to implement a state encoder, which maps images into latent vectors. This will be \n", "The desired architecture is very similar to the VAE encoder you implemented in the previosu question, but is now deterministic. The architecture is as follows:\n", "\n", "![state_encoder_architecture](https://drive.google.com/uc?export=view&id=1zsECGD0SPXXfeYY-sx5X8wJcVk-HhiKe)\n", "\n", "Next, you will have the dynamics model in the latent space. You should use exactly the same residual dynamics architecture you have used in both HW3 and HW4 for this. \n", "\n", "Finally there is the decoder. For the decoder we will re-use the decoder you implemented for the VAE. \n", "\n", "\n", "### Loss Function\n", "\n", "We will use a multi-step loss function to train the autoencoder and the latent dynamics model end-to-end. A single training trajectory is given by input sequence of images $x_0, x_1, ..., x_T$, and controls $a_0, ..., a_{T-1}$. We encoder **every** image to latent vector $z$ with the encoder to produce $z_0, z_1, ..., z_T$. In other words, $z_{t} = \\texttt{encode}(x_t)$.\n", "\n", "From the initial latent state $z_0$ we apply the latent dynamics model **recursively**, i.e. $\\hat{z}_{1} = f_{latent}(z_0, a_0)$, $\\hat{z}_{2} = f_{latent}(\\hat{z}_1, a_1)$ and so on. This produces predictions $\\hat{z}_1, ..., \\hat{z}_T$. Similarly, we do the same for states. Starting at $x_0$, we propagate state dynamics to produce $\\hat{x}_1, ..., \\hat{x}_T$, where $\\hat{x}_{t+1} = f_\\text{dyn}(\\hat{x}_t, a_t)$, and $\\hat{x}_{1} = f_\\text{dyn}(x_0, a_t)$.\n", "\n", "The full loss is then:\n", "\n", "$$\n", "    \\frac{1}{T+1}\\sum^T_{t=0} ||x_t - \\texttt{decode}(z_t)||^2 + \\frac{1}{T}\\sum_{t=1}^T ||x_t - \\hat x_{t}||^2 + \\frac{\\alpha}{T} \\sum^T_{t=1} ||z_t - \\hat{z}_t||^2\n", "$$\n", "\n", "$\\alpha$ is a hyperparameter and should be kept as 0.1. This loss term has three components, the first is a standard reconstruction loss. The second two losses are both losses on the latent dynamics prediction. \n", "\n", "**TODO:**\n", "* Implement `StateEncoder` in `learning_latent_dynamics.py`\n", "    * Need to implement `__init__` and `forward` methods\n", "* Implement `LatentDynamicsModel` in `learning_latent_dynamics.py`\n", "    * Need to implement the methods `__init__`, `forward`, `encode`, `decode` and `latent_dynamics`.\n", "* Implement `MultiStepLoss` in `learning_latent_dynamics.py`\n", "    * Need to implement the `forward`.\n", "    * Your MultiStepLoss should work for arbitrary `num_steps` in the dataset, including `num_steps=1`, i.e. a single-step loss.\n", "    \n", "    "], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4.1 - Learn Single Step Latent Dynamics  (15 points)\n", "\n", "First you will train the model with a single-step loss. This is acheived by simply passing `num_steps=1` into `process_data_multiple_step`. \n", "\n", "You should be able to train the model to have validation loss of <0.25. You will need to submit your model `single_step_latent_dynamics_model.pt` on autograder. "], "outputs": []}, {"cell_type": "code", "execution_count": 13, "metadata": {}, "outputs": [], "source": ["# Train the dynamics model\n", "\n", "LATENT_DIM = 16\n", "ACTION_DIM = 3\n", "NUM_CHANNELS = 1\n", "NUM_STEPS = 1\n", "\n", "single_step_latent_dynamics_model = LatentDynamicsModel(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n", "\n", "\n", "# Compute normalization constants\n", "train_loader, val_loader, norm_constants = process_data_multiple_step(collected_data, batch_size=500, num_steps=NUM_STEPS)\n", "norm_tr = NormalizationTransform(norm_constants)\n", "\n", "state_loss_fn = nn.MSELoss()\n", "latent_loss_fn = nn.MSELoss()\n", "multistep_loss = MultiStepLoss(state_loss_fn, latent_loss_fn, alpha=0.1)\n", "\n", "# TODO: Train latent_dynamics_model\n", "# --- Your code here\n", "\n\n\n", "# ---\n", "\n", "# save model:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'single_step_latent_dynamics_model.pt')\n", "torch.save(single_step_latent_dynamics_model.state_dict(), save_path)"]}, {"cell_type": "code", "execution_count": 14, "metadata": {}, "outputs": [], "source": ["# Visualize the ability to perform  state and its reconstruction:\n", "traj = collected_data[0]\n", "evaluate_model_plot(single_step_latent_dynamics_model, traj, norm_tr)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Observe how the predicted image gets out of distribution with the single-step training."], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4.2 - Learn Multi-Step Dynamics  (15 points)\n", "\n", "Now we will train using a multi-step loss by `num_steps=4` into `process_data_multiple_step`. \n", "\n", "You should be able to train the model to have validation loss of <0.25. You will need to submit your model `multi_step_latent_dynamics_model.pt` on autograder. \n"], "outputs": []}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": ["# Train the dynamics model\n", "LATENT_DIM = 16\n", "ACTION_DIM = 3\n", "NUM_CHANNELS = 1\n", "NUM_STEPS = 4\n", "\n", "multi_step_latent_dynamics_model = LatentDynamicsModel(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n", "\n", "state_loss_fn = nn.MSELoss()\n", "latent_loss_fn = nn.MSELoss()\n", "multistep_loss = MultiStepLoss(state_loss_fn, latent_loss_fn, alpha=0.1)\n", "\n", "# Compute normalization constants\n", "train_loader, val_loader, norm_constants = process_data_multiple_step(collected_data, batch_size=512, num_steps=NUM_STEPS)\n", "\n", "\n", "# --- Your code here\n", "\n\n\n", "# ---\n", "\n", "# save model:\n", "save_path = os.path.join(GOOGLE_DRIVE_PATH, 'multi_step_latent_dynamics_model.pt')\n", "torch.save(multi_step_latent_dynamics_model.state_dict(), save_path)"]}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": ["# Visualize the ability to perform  state and its reconstruction:\n", "traj = collected_data[0]\n", "evaluate_model_plot(multi_step_latent_dynamics_model, traj, norm_tr)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Single-Step vs Multi-Step Loss\n", "\n", "Now we will evalaute the difference between the multi-step model and the single-step model. You should see that the multi-step trained model has similar or better performance on the single-step loss but better improvement on the multi-step loss.\n", "\n", "**Grading Details**: We will test the performance of your model on `pushing_image_validation_data.npy`. You need to obtain at least the following losses values:\n", "* Single-step model evaluated on single-step loss: 0.25\n", "* Single-step model evaluated on multi-step loss: 0.37\n", "* Multi-step model evaluated on single-step loss: 0.23\n", "* Multi-step model evaluated on multi-step loss: 0.33"], "outputs": []}, {"cell_type": "code", "execution_count": 18, "metadata": {}, "outputs": [], "source": ["test_data = np.load(os.path.join(GOOGLE_DRIVE_PATH, 'pushing_image_validation_data.npy'), allow_pickle=True)\n", "norm_tr = NormalizationTransform(norm_constants)\n", "\n", "single_step_dataset = MultiStepDynamicsDataset(test_data, num_steps=1, transform=norm_tr)\n", "single_step_loader = DataLoader(single_step_dataset, batch_size=len(single_step_dataset))\n", "\n", "multi_step_dataset = MultiStepDynamicsDataset(test_data, num_steps=4, transform=norm_tr)\n", "multi_step_loader = DataLoader(multi_step_dataset, batch_size=len(multi_step_dataset))\n", "\n", "loss = MultiStepLoss(state_loss_fn, latent_loss_fn, alpha=1)\n", "\n", "def test(model, val_loader, loss_fn) -> float:\n", "    \"\"\"\n", "    Perfoms an epoch of model performance validation\n", "    :param model: Pytorch nn.Module\n", "    :param train_loader: Pytorch DataLoader\n", "    :param loss: Loss function\n", "    :return: val_loss <float> representing the average loss among the different mini-batches\n", "    \"\"\"\n", "    val_loss = 0. # TODO: Modify the value\n", "    # Initialize the validation loop\n", "    model.eval()\n", "    for batch in val_loader:\n", "        loss = None\n", "        states = batch['states']\n", "        actions = batch['actions']\n", "        loss = loss_fn(model, states, actions)\n", "        val_loss += loss.item()\n", "    return val_loss/len(val_loader)\n", "\n", "\n", "print(f'Single-step model evaluated on single-step loss: {test(single_step_latent_dynamics_model, single_step_loader, loss)}')\n", "print(f'Single-step model evaluated on multi-step loss: {test(single_step_latent_dynamics_model, multi_step_loader, loss)}')\n", "print('')\n", "print(f'Multi-step model evaluated on single-step loss: {test(multi_step_latent_dynamics_model, single_step_loader, loss)}')\n", "print(f'Multi-step model evaluated on multi-step loss: {test(multi_step_latent_dynamics_model, multi_step_loader, loss)}')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 5 - Latent Space Control (MPPI)  (30 points)\n", "\n", "For this last part, we will use the learned model to drive the block to a goal location from image states and latent dynamics model."], "outputs": []}, {"cell_type": "code", "execution_count": 19, "metadata": {}, "outputs": [], "source": ["# Load the model\n", "latent_dynamics_model = LatentDynamicsModel(latent_dim=LATENT_DIM, action_dim=ACTION_DIM, num_channels=NUM_CHANNELS)\n", "model_path = os.path.join(GOOGLE_DRIVE_PATH, 'multi_step_latent_dynamics_model.pt')\n", "latent_dynamics_model.load_state_dict(torch.load(model_path))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5.1 Latent Space Controller (15 points)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["In this part we will use the learned latent dynamics model to formulate a control problem in latent space.\n", "\n", "MPPI will propagate the dyanamics in the latent space. Moreover, the cost function will be formulated in latent space, comparing how close states are to the goal in latent space.\n", "\n", "**TODO**:\n", "* Implement `latent_space_pushing_cost_function` in `learning_latent_dynamics.py`.\n", "* Implement the missing part from `PushingLatentController` in `learning_latent_dynamics.py`. In particular you need to implement the following methods:\n", "* \n", "* Implement the missing part from `PushingLatentController` in `learning_latent_dynamics.py`. In particular you need to implement the following methods:\n", "    * `_compute_dynamics`\n", "    * `_compute_costs`\n", "    * `control`\n", "    * `_wrap_state`\n", "    * `_unwrap_state`\n", "    \n", "**GRADING GUIDE**:\n", "We will execute your controller 10 times on the same configuration provided below.  At least one needs to reach the goal."], "outputs": []}, {"cell_type": "code", "execution_count": 20, "metadata": {}, "outputs": [], "source": ["# Control on an obstacle free environment\n", "%matplotlib inline\n", "\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "\n", "target_state = np.array([0.7, 0., 0.])\n", "\n", "env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True)\n", "state_0 = env.reset()\n", "env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n", "controller = PushingLatentController(env, latent_dynamics_model, latent_space_pushing_cost_function,norm_constants, num_samples=100, horizon=10)\n", "\n", "state = state_0\n", "\n", "# num_steps_max = 100\n", "num_steps_max = 20\n", "\n", "for i in range(num_steps_max):\n", "    action = controller.control(state)\n", "    state, reward, done, _ = env.step(action)\n", "    # check if we have reached the goal\n", "    end_pose = env.get_object_pos_planar()\n", "    goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n", "    goal_reached = goal_distance < BOX_SIZE\n", "    if done or goal_reached:\n", "        break\n", "\n", "print(f'GOAL REACHED: {goal_reached}')\n", "        \n", "plt.close(fig)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5.2 - Image Space Controller (15 points)"], "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["In this final part, we will reformulate the controller but to work on image space. MPPI will propagate the dynamics in image space and the costs will also be computed in the image space. We will use $\\text{MSE}(\\mathrm x_t, \\mathrm x_g)$ to compute how close a state $\\mathbf x_t$ is from the goal state $\\mathrm x_g$.\n", "\n", "**TODO**:\n", "* Implement `img_space_pushing_cost_function` in `learning_latent_dynamics.py`.\n", "* Implement the missing part from `PushingImgSpaceController` in `learning_latent_dynamics.py`. In particular you need to implement the following methods:\n", "    * `_compute_dynamics`\n", "    * `_compute_costs`\n", "    * `control`\n", "    * `_wrap_state`\n", "    * `_unwrap_state`\n", "    \n", "**GRADING GUIDE**:\n", "We will execute your controller 10 times on the same configuration provided below.  At least one needs to reach the goal."], "outputs": []}, {"cell_type": "code", "execution_count": 21, "metadata": {}, "outputs": [], "source": ["# Control on an obstacle free environment\n", "%matplotlib inline\n", "\n", "fig = plt.figure(figsize=(8,8))\n", "hfig = display(fig, display_id=True)\n", "visualizer = NotebookVisualizer(fig=fig, hfig=hfig)\n", "\n", "\n", "target_state = np.array([0.7, 0., 0.])\n", "\n", "env = PandaImageSpacePushingEnv(visualizer=visualizer, render_non_push_motions=False,  camera_heigh=800, camera_width=800, render_every_n_steps=5, grayscale=True)\n", "state_0 = env.reset()\n", "env.object_target_pose = env._planar_pose_to_world_pose(target_state)\n", "controller = PushingImgSpaceController(env, latent_dynamics_model, img_space_pushing_cost_function, norm_constants, num_samples=100, horizon=10)\n", "\n", "state = state_0\n", "\n", "# num_steps_max = 100\n", "num_steps_max = 30\n", "\n", "\n", "goal_reached = False\n", "for i in range(num_steps_max):\n", "    action = controller.control(state)\n", "    state, reward, done, _ = env.step(action)\n", "    \n", "    # check if we have reached the goal\n", "    end_pose = env.get_object_pos_planar()\n", "    goal_distance = np.linalg.norm(end_pose[:2]-target_state[:2]) # evaluate only position, not orientation\n", "    goal_reached = goal_distance < BOX_SIZE\n", "    if done or goal_reached:\n", "        break\n", "\n", "\n", "print(f'GOAL REACHED: {goal_reached}')\n", "        \n", "        \n", "plt.close(fig)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Congratulations, you have reached the end of this homework. Nice work! \n", "Please, submit all required files to [autograder.io](https://autograder.io)"], "outputs": []}], "metadata": {"colab": {"authorship_tag": "ABX9TyNRQ/BgPmM29eFX3MyfIYKz", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 1}